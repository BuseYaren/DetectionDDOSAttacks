{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99a2f066",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment1:\n",
    "    \n",
    "    def __init__(self, data, history_t=90):\n",
    "        self.data = data\n",
    "        self.history_t = history_t\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.done = False\n",
    "        self.profits = 0\n",
    "        self.positions = []\n",
    "        self.position_value = 0\n",
    "        self.history = [0 for _ in range(self.history_t)]\n",
    "        return [self.position_value] + self.history # obs\n",
    "    \n",
    "    def step(self, dos):\n",
    "        reward = 0 \n",
    "        if dos == 1:   #saldırıya uğramış ise\n",
    "            self.positions.append(self.data.iloc[self.t, :]['Label'])\n",
    "        else:     #saldırıya uğramamış\n",
    "            reward = -1\n",
    "        # dos = 0: not attacked, dos=1: attack\n",
    "        # set next time\n",
    "        self.t += 1\n",
    "        self.position_value = 0\n",
    "        for p in self.positions:\n",
    "            self.position_value += (self.data.iloc[self.t, :]['Label'] - p)\n",
    "        self.history.pop(0)\n",
    "        self.history.append(self.data.iloc[self.t, :]['Label'] - self.data.iloc[(self.t-1), :]['Label'])\n",
    "        \n",
    "        # clipping reward\n",
    "        if reward > 0:\n",
    "            reward = 1\n",
    "        elif reward < 0:\n",
    "            reward = -1\n",
    "        \n",
    "        return [self.position_value] + self.history, reward, self.done # obs, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2641aaad",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-00940db99a80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvironment1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "env = Environment1(train)\n",
    "print(env.reset())\n",
    "for _ in range(3):\n",
    "    pact = np.random.randint(3)\n",
    "    print(env.step(pact))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2c3096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(env):\n",
    "\n",
    "    class Q_Network(chainer.Chain):\n",
    "\n",
    "        def __init__(self, input_size, hidden_size, output_size):\n",
    "            super(Q_Network, self).__init__(\n",
    "                fc1 = L.Linear(input_size, hidden_size),\n",
    "                fc2 = L.Linear(hidden_size, hidden_size),\n",
    "                fc3 = L.Linear(hidden_size, output_size)\n",
    "            )\n",
    "\n",
    "        def __call__(self, x):\n",
    "            h = F.relu(self.fc1(x))\n",
    "            h = F.relu(self.fc2(h))\n",
    "            y = self.fc3(h)\n",
    "            return y\n",
    "\n",
    "        def reset(self):\n",
    "            self.zerograds()\n",
    "\n",
    "    Q = Q_Network(input_size=env.history_t+1, hidden_size=100, output_size=3)\n",
    "    Q_ast = copy.deepcopy(Q)\n",
    "    optimizer = chainer.optimizers.Adam()\n",
    "    optimizer.setup(Q)\n",
    "\n",
    "    epoch_num = 50\n",
    "    step_max = len(env.data)-1\n",
    "    memory_size = 200\n",
    "    batch_size = 20\n",
    "    epsilon = 1.0\n",
    "    epsilon_decrease = 1e-3\n",
    "    epsilon_min = 0.1\n",
    "    start_reduce_epsilon = 200\n",
    "    train_freq = 10\n",
    "    update_q_freq = 20\n",
    "    gamma = 0.97\n",
    "    show_log_freq = 5\n",
    "\n",
    "    memory = []\n",
    "    total_step = 0\n",
    "    total_rewards = []\n",
    "    total_losses = []\n",
    "\n",
    "    start = time.time()\n",
    "    for epoch in range(epoch_num):\n",
    "\n",
    "        pobs = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        total_loss = 0\n",
    "\n",
    "        while not done and step < step_max:\n",
    "\n",
    "            # select act\n",
    "            pact = np.random.randint(3)\n",
    "            if np.random.rand() > epsilon:\n",
    "                pact = Q(np.array(pobs, dtype=np.float32).reshape(1, -1))\n",
    "                pact = np.argmax(pact.data)\n",
    "\n",
    "            # act\n",
    "            obs, reward, done = env.step(pact)\n",
    "\n",
    "            # add memory\n",
    "            memory.append((pobs, pact, reward, obs, done))\n",
    "            if len(memory) > memory_size:\n",
    "                memory.pop(0)\n",
    "\n",
    "            # train or update q\n",
    "            if len(memory) == memory_size:\n",
    "                if total_step % train_freq == 0:\n",
    "                    shuffled_memory = np.random.permutation(memory)\n",
    "                    memory_idx = range(len(shuffled_memory))\n",
    "                    for i in memory_idx[::batch_size]:\n",
    "                        batch = np.array(shuffled_memory[i:i+batch_size])\n",
    "                        b_pobs = np.array(batch[:, 0].tolist(), dtype=np.float32).reshape(batch_size, -1)\n",
    "                        b_pact = np.array(batch[:, 1].tolist(), dtype=np.int32)\n",
    "                        b_reward = np.array(batch[:, 2].tolist(), dtype=np.int32)\n",
    "                        b_obs = np.array(batch[:, 3].tolist(), dtype=np.float32).reshape(batch_size, -1)\n",
    "                        b_done = np.array(batch[:, 4].tolist(), dtype=np.bool)\n",
    "\n",
    "                        q = Q(b_pobs)\n",
    "                        maxq = np.max(Q_ast(b_obs).data, axis=1)\n",
    "                        target = copy.deepcopy(q.data)\n",
    "                        for j in range(batch_size):\n",
    "                            target[j, b_pact[j]] = b_reward[j]+gamma*maxq[j]*(not b_done[j])\n",
    "                        Q.reset()\n",
    "                        loss = F.mean_squared_error(q, target)\n",
    "                        total_loss += loss.data\n",
    "                        loss.backward()\n",
    "                        optimizer.update()\n",
    "\n",
    "                if total_step % update_q_freq == 0:\n",
    "                    Q_ast = copy.deepcopy(Q)\n",
    "\n",
    "            # epsilon\n",
    "            if epsilon > epsilon_min and total_step > start_reduce_epsilon:\n",
    "                epsilon -= epsilon_decrease\n",
    "\n",
    "            # next step\n",
    "            total_reward += reward\n",
    "            pobs = obs\n",
    "            step += 1\n",
    "            total_step += 1\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "        total_losses.append(total_loss)\n",
    "\n",
    "        if (epoch+1) % show_log_freq == 0:\n",
    "            log_reward = sum(total_rewards[((epoch+1)-show_log_freq):])/show_log_freq\n",
    "            log_loss = sum(total_losses[((epoch+1)-show_log_freq):])/show_log_freq\n",
    "            elapsed_time = time.time()-start\n",
    "            print('\\t'.join(map(str, [epoch+1, epsilon, total_step, log_reward, log_loss, elapsed_time])))\n",
    "            start = time.time()\n",
    "            \n",
    "    return Q, total_losses, total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18328b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, total_losses, total_rewards = train_dqn(Environment1(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68494b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_reward(total_losses, total_rewards):\n",
    "\n",
    "    figure = tools.make_subplots(rows=1, cols=2, subplot_titles=('loss', 'reward'), print_grid=False)\n",
    "    figure.append_trace(Scatter(y=total_losses, mode='lines', line=dict(color='skyblue')), 1, 1)\n",
    "    figure.append_trace(Scatter(y=total_rewards, mode='lines', line=dict(color='orange')), 1, 2)\n",
    "    figure['layout']['xaxis1'].update(title='epoch')\n",
    "    figure['layout']['xaxis2'].update(title='epoch')\n",
    "    figure['layout'].update(height=400, width=900, showlegend=False)\n",
    "    iplot(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bda9c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_reward(total_losses, total_rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
